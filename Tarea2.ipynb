{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 AID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bruno Benkel - 201204111-2\n",
    "- Felipe Chacon - 201303017-3\n",
    "- Diego Wilhelm - 201303059-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from statsmodels.sandbox.tools.tools_pca import pcasvd\n",
    "from sklearn.neighbors import kneighbors_graph # Técnica de k-Vecino mas cercano\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats, integrate\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hepatitis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Class: DIE, LIVE\n",
    " 2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
    " 3. SEX: male, female\n",
    " 4. STEROID: no, yes\n",
    " 5. ANTIVIRALS: no, yes\n",
    " 6. FATIGUE: no, yes\n",
    " 7. MALAISE: no, yes\n",
    " 8. ANOREXIA: no, yes\n",
    " 9. LIVER BIG: no, yes\n",
    " 10. LIVER FIRM: no, yes\n",
    " 11. SPLEEN PALPABLE: no, yes\n",
    " 12. SPIDERS: no, yes\n",
    " 13. ASCITES: no, yes\n",
    " 14. VARICES: no, yes\n",
    " 15. BILIRUBIN: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
    " 16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
    " 17. SGOT: 13, 100, 200, 300, 400, 500, \n",
    " 18. ALBUMIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
    " 19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
    " 20. HISTOLOGY: no, yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/hepatitis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for i in range(df1.shape[0]):\n",
    "    if('?' in df1.iloc[i].values):\n",
    "        index.append(i)\n",
    "df1 = df1.drop(index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sacar la columna Class, que es la que se intenta predecir\n",
    "target1 = df1['Class']\n",
    "df1 = df1.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = df1.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Fueron utilizados K-Means, Average Linkage y Mean Shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.482762\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cityblock', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='average', memory=None,\n",
       "            n_clusters=2, pooling_func=<function mean at 0x7f5454415400>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=2)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.671057\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=59.752845675029263, bin_seeding=True, cluster_all=True,\n",
       "     min_bin_freq=1, n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.368808\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador 1] Accuracy: 0.750  -  Precision: 0.60  -  F Score: 0.609\n",
      "[Clasificador 2] Accuracy: 0.850  -  Precision: 0.42  -  F Score: 0.459\n",
      "[Clasificador 3] Accuracy: 0.875  -  Precision: 0.76  -  F Score: 0.736\n"
     ]
    }
   ],
   "source": [
    "H_entrenamiento, H_test, target1_entrenamiento, target1_test = train_test_split(df1, target1, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc1 = StandardScaler()\n",
    "sc1.fit(H_entrenamiento) \n",
    "Z1_entrenamiento = sc1.transform(H_entrenamiento)\n",
    "Z1_test = sc1.transform(H_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred1 = clasificador1.predict(Z1_test)\n",
    "accuracy1 = accuracy_score(target1_test, target1_pred1)\n",
    "precision1 = precision_score(target1_test, target1_pred1, average=\"macro\")\n",
    "F1 = f1_score(target1_test, target1_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred2 = clasificador2.predict(Z1_test)\n",
    "accuracy2 = accuracy_score(target1_test, target1_pred2)\n",
    "precision2 = precision_score(target1_test, target1_pred2, average=\"macro\")\n",
    "F2 = f1_score(target1_test, target1_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred3 = clasificador3.predict(Z1_test)\n",
    "accuracy3 = accuracy_score(target1_test, target1_pred3)\n",
    "precision3 = precision_score(target1_test, target1_pred3, average=\"macro\")\n",
    "F3 = f1_score(target1_test, target1_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador1] Mean: 0.761 - Standar Deviation: 0.181\n",
      "[Clasificador2] Mean: 0.840 - Standar Deviation: 0.042\n",
      "[Clasificador3] Mean: 0.814 - Standar Deviation: 0.057\n"
     ]
    }
   ],
   "source": [
    "scores1 = cross_val_score(clasificador1, df1, target1, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df1, target1, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df1, target1, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.astype(float)\n",
    "df1 = (df1 - df1.mean()) / df1.std()\n",
    "pca1 = pcasvd(df1, keepdim=15, demean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador 1] Accuracy: 0.675  -  Precision: 0.56  -  F Score: 0.551\n",
      "[Clasificador 2] Accuracy: 0.850  -  Precision: 0.42  -  F Score: 0.459\n",
      "[Clasificador 3] Accuracy: 0.800  -  Precision: 0.56  -  F Score: 0.543\n"
     ]
    }
   ],
   "source": [
    "H_entrenamiento, H_test, target1_entrenamiento, target1_test = train_test_split(pca1[0], target1, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc1 = StandardScaler()\n",
    "sc1.fit(H_entrenamiento) \n",
    "Z1_entrenamiento = sc1.transform(H_entrenamiento)\n",
    "Z1_test = sc1.transform(H_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred1 = clasificador1.predict(Z1_test)\n",
    "accuracy1 = accuracy_score(target1_test, target1_pred1)\n",
    "precision1 = precision_score(target1_test, target1_pred1, average=\"macro\")\n",
    "F1 = f1_score(target1_test, target1_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred2 = clasificador2.predict(Z1_test)\n",
    "accuracy2 = accuracy_score(target1_test, target1_pred2)\n",
    "precision2 = precision_score(target1_test, target1_pred2, average=\"macro\")\n",
    "F2 = f1_score(target1_test, target1_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred3 = clasificador3.predict(Z1_test)\n",
    "accuracy3 = accuracy_score(target1_test, target1_pred3)\n",
    "precision3 = precision_score(target1_test, target1_pred3, average=\"macro\")\n",
    "F3 = f1_score(target1_test, target1_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador1] Mean: 0.761 - Standar Deviation: 0.181\n",
      "[Clasificador2] Mean: 0.840 - Standar Deviation: 0.042\n",
      "[Clasificador3] Mean: 0.837 - Standar Deviation: 0.079\n"
     ]
    }
   ],
   "source": [
    "scores1 = cross_val_score(clasificador1, df1, target1, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df1, target1, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df1, target1, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indian Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/diabetes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for i in range(df2.shape[0]):\n",
    "    if('?' in df2.iloc[i].values):\n",
    "        index.append(i)\n",
    "df2 = df2.drop(index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target2 = df2['Class']\n",
    "df2 = df2.drop('Class',axis=1)\n",
    "mat = df2.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.568790\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cityblock', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='average', memory=None,\n",
       "            n_clusters=2, pooling_func=<function mean at 0x7f5454415400>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=2)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.728518\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=63.883927652192064, bin_seeding=True, cluster_all=True,\n",
       "     min_bin_freq=1, n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.428159\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador 1] Accuracy: 0.781  -  Precision: 0.77  -  F Score: 0.741\n",
      "[Clasificador 2] Accuracy: 0.766  -  Precision: 0.74  -  F Score: 0.728\n",
      "[Clasificador 3] Accuracy: 0.789  -  Precision: 0.79  -  F Score: 0.743\n"
     ]
    }
   ],
   "source": [
    "D_entrenamiento, D_test, target2_entrenamiento, target2_test = train_test_split(df2, target2, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "sc2.fit(D_entrenamiento) \n",
    "Z2_entrenamiento = sc2.transform(D_entrenamiento)\n",
    "Z2_test = sc2.transform(D_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred1 = clasificador1.predict(Z2_test)\n",
    "accuracy1 = accuracy_score(target2_test, target2_pred1)\n",
    "precision1 = precision_score(target2_test, target2_pred1, average=\"macro\")\n",
    "F1 = f1_score(target2_test, target2_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred2 = clasificador2.predict(Z2_test)\n",
    "accuracy2 = accuracy_score(target2_test, target2_pred2)\n",
    "precision2 = precision_score(target2_test, target2_pred2, average=\"macro\")\n",
    "F2 = f1_score(target2_test, target2_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred3 = clasificador3.predict(Z2_test)\n",
    "accuracy3 = accuracy_score(target2_test, target2_pred3)\n",
    "precision3 = precision_score(target2_test, target2_pred3, average=\"macro\")\n",
    "F3 = f1_score(target2_test, target2_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador1] Mean: 0.773 - Standar Deviation: 0.035\n",
      "[Clasificador2] Mean: 0.740 - Standar Deviation: 0.028\n",
      "[Clasificador3] Mean: 0.766 - Standar Deviation: 0.025\n"
     ]
    }
   ],
   "source": [
    "#KFold\n",
    "scores1 = cross_val_score(clasificador1, df2, target2, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df2, target2, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df2, target2, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.astype(float)\n",
    "df2 = (df2 - df2.mean()) / df2.std()\n",
    "pca2 = pcasvd(df2, keepdim=6, demean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador 1] Accuracy: 0.781  -  Precision: 0.77  -  F Score: 0.741\n",
      "[Clasificador 2] Accuracy: 0.688  -  Precision: 0.64  -  F Score: 0.626\n",
      "[Clasificador 3] Accuracy: 0.781  -  Precision: 0.77  -  F Score: 0.738\n"
     ]
    }
   ],
   "source": [
    "D_entrenamiento, D_test, target2_entrenamiento, target2_test = train_test_split(pca2[0], target2, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "sc2.fit(D_entrenamiento) \n",
    "Z2_entrenamiento = sc2.transform(D_entrenamiento)\n",
    "Z2_test = sc2.transform(D_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred1 = clasificador1.predict(Z2_test)\n",
    "accuracy1 = accuracy_score(target2_test, target2_pred1)\n",
    "precision1 = precision_score(target2_test, target2_pred1, average=\"macro\")\n",
    "F1 = f1_score(target2_test, target2_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred2 = clasificador2.predict(Z2_test)\n",
    "accuracy2 = accuracy_score(target2_test, target2_pred2)\n",
    "precision2 = precision_score(target2_test, target2_pred2, average=\"macro\")\n",
    "F2 = f1_score(target2_test, target2_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred3 = clasificador3.predict(Z2_test)\n",
    "accuracy3 = accuracy_score(target2_test, target2_pred3)\n",
    "precision3 = precision_score(target2_test, target2_pred3, average=\"macro\")\n",
    "F3 = f1_score(target2_test, target2_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador1] Mean: 0.773 - Standar Deviation: 0.035\n",
      "[Clasificador2] Mean: 0.740 - Standar Deviation: 0.028\n",
      "[Clasificador3] Mean: 0.771 - Standar Deviation: 0.033\n"
     ]
    }
   ],
   "source": [
    "#KFold\n",
    "scores1 = cross_val_score(clasificador1, df2, target2, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df2, target2, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df2, target2, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Treatment Plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|N. | Identificador | Descripción |\n",
    "| -- |:---:|:---|\n",
    "| 1 | Q-E       | (input flow to plant)   |\n",
    "| 2 | ZN-E      | (input Zinc to plant) |\n",
    "| 3 | PH-E      | (input pH to plant)  |\n",
    "| 4 | DBO-E     | (input Biological demand of oxygen to plant)  |\n",
    "| 5 | DQO-E     | (input chemical demand of oxygen to plant) |\n",
    "| 6 | SS-E      | (input suspended solids to plant)   |\n",
    "| 7 | SSV-E     | (input volatile supended solids to plant) |\n",
    "| 8 | SED-E     | (input sediments to plant)  |\n",
    "| 9 | COND-E    | (input conductivity to plant)  |\n",
    "|10 | PH-P      | (input pH to primary settler) |\n",
    "|11 | DBO-P     | (input Biological demand of oxygen to primary settler) |\n",
    "|12 | SS-P      | (input suspended solids to primary settler) |\n",
    "|13 | SSV-P     | (input volatile supended solids to primary settler) |\n",
    "|14 | SED-P     | (input sediments to primary settler)  |\n",
    "|15 | COND-P    | (input conductivity to primary settler) |\n",
    "|16 | PH-D      | (input pH to secondary settler)  |\n",
    "|17 | DBO-D     | (input Biological demand of oxygen to secondary settler) |\n",
    "|18 | DQO-D     | (input chemical demand of oxygen to secondary settler) |\n",
    "|19 | SS-D      | (input suspended solids to secondary settler) |\n",
    "|20 | SSV-D     | (input volatile supended solids to secondary settler) |\n",
    "|21 | SED-D     | (input sediments to secondary settler)   |\n",
    "|22 | COND-D    | (input conductivity to secondary settler)  |\n",
    "|23 | PH-S      | (output pH)    |\n",
    "|24 | DBO-S     | (output Biological demand of oxygen) |\n",
    "|25 | DQO-S     | (output chemical demand of oxygen) |\n",
    "|26 | SS-S      | (output suspended solids) |\n",
    "|27 | SSV-S     | (output volatile supended solids)  |\n",
    "|28 | SED-S     | (output sediments)  |\n",
    "|29 | COND-S    | (output conductivity) |\n",
    "|30 | RD-DBO-P  | (performance input Biological demand of oxygen in primary settler) |\n",
    "|31 | RD-SS-P   | (performance input suspended solids to primary settler) |\n",
    "|32 | RD-SED-P  | (performance input sediments to primary settler) |\n",
    "|33 | RD-DBO-S  | (performance input Biological demand of oxygen to secondary settler) |\n",
    "|34 | RD-DQO-S  | (performance input chemical demand of oxygen to secondary settler) |\n",
    "|35 | RD-DBO-G  | (global performance input Biological demand of oxygen) |\n",
    "|36 | RD-DQO-G  | (global performance input chemical demand of oxygen) |\n",
    "|37 | RD-SS-G   | (global performance input suspended solids)  |\n",
    "|38 | RD-SED-G  | (global performance input sediments) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 13 clases a las que puede pertenecer esto\n",
    "\n",
    "* Class 1: Normal situation\n",
    "* Class 2: Secondary settler problems-1\n",
    "* Class 3: Secondary settler problems-2\n",
    "* Class 4: Secondary settler problems-3\n",
    "* Class 5: Normal situation with performance over the mean\n",
    "* Class 6: Solids overload-1\n",
    "* Class 7: Secondary settler problems-4\n",
    "* Class 8: Storm-1\n",
    "* Class 9: Normal situation with a low influent\n",
    "* Class 10: Storm-2\n",
    "* Class 11: Normal situation\n",
    "* Class 12: Storm-3\n",
    "* Class 13: Solids overload-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/water-treatment.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for j in df3.index:\n",
    "    if('?' in df3.loc[j].values):\n",
    "        index.append(j)\n",
    "df3 = df3.drop(index)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "target3 = np.random.randint(2,13,size=380)\n",
    "#dataset no incluye el atributo de la clase\n",
    "mat = df3.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=13, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=13)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.381638\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgglomerativeClustering(affinity='cityblock', compute_full_tree='auto',\n",
       "            connectivity=None, linkage='average', memory=None,\n",
       "            n_clusters=13, pooling_func=<function mean at 0x7f5454415400>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=13)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.342311\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MeanShift(bandwidth=2703.5498315775735, bin_seeding=True, cluster_all=True,\n",
       "     min_bin_freq=1, n_jobs=1, seeds=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.512379\n"
     ]
    }
   ],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Clasificador 1] Accuracy: 0.116  -  Precision: 0.10  -  F Score: 0.103\n",
      "[Clasificador 2] Accuracy: 0.095  -  Precision: 0.11  -  F Score: 0.095\n",
      "[Clasificador 3] Accuracy: 0.084  -  Precision: 0.11  -  F Score: 0.078\n"
     ]
    }
   ],
   "source": [
    "W_entrenamiento, W_test, target3_entrenamiento, target3_test = train_test_split(df3, target3, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "sc3.fit(W_entrenamiento) \n",
    "Z3_entrenamiento = sc3.transform(W_entrenamiento)\n",
    "Z3_test = sc3.transform(W_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred1 = clasificador1.predict(Z3_test)\n",
    "accuracy1 = accuracy_score(target3_test, target3_pred1)\n",
    "precision1 = precision_score(target3_test, target3_pred1, average=\"macro\")\n",
    "F1 = f1_score(target3_test, target3_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred2 = clasificador2.predict(Z3_test)\n",
    "accuracy2 = accuracy_score(target3_test, target3_pred2)\n",
    "precision2 = precision_score(target3_test, target3_pred2, average=\"macro\")\n",
    "F2 = f1_score(target3_test, target3_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred3 = clasificador3.predict(Z3_test)\n",
    "accuracy3 = accuracy_score(target3_test, target3_pred3)\n",
    "precision3 = precision_score(target3_test, target3_pred3, average=\"macro\")\n",
    "F3 = f1_score(target3_test, target3_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KFold\n",
    "scores1 = cross_val_score(clasificador1, df3, target3, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df3, target3, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df3, target3, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df3.astype(float)\n",
    "df3 = (df3 - df3.mean()) / df3.std()\n",
    "pca3 = pcasvd(df3, keepdim=30, demean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_entrenamiento, W_test, target3_entrenamiento, target3_test = train_test_split(pca3[0], target3, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "sc3.fit(W_entrenamiento) \n",
    "Z3_entrenamiento = sc3.transform(W_entrenamiento)\n",
    "Z3_test = sc3.transform(W_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred1 = clasificador1.predict(Z3_test)\n",
    "accuracy1 = accuracy_score(target3_test, target3_pred1)\n",
    "precision1 = precision_score(target3_test, target3_pred1, average=\"macro\")\n",
    "F1 = f1_score(target3_test, target3_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred2 = clasificador2.predict(Z3_test)\n",
    "accuracy2 = accuracy_score(target3_test, target3_pred2)\n",
    "precision2 = precision_score(target3_test, target3_pred2, average=\"macro\")\n",
    "F2 = f1_score(target3_test, target3_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred3 = clasificador3.predict(Z3_test)\n",
    "accuracy3 = accuracy_score(target3_test, target3_pred3)\n",
    "precision3 = precision_score(target3_test, target3_pred3, average=\"macro\")\n",
    "F3 = f1_score(target3_test, target3_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KFold\n",
    "scores1 = cross_val_score(clasificador1, df3, target3, cv=10, scoring='accuracy')\n",
    "scores2 = cross_val_score(clasificador2, df3, target3, cv=10, scoring='accuracy')\n",
    "scores3 = cross_val_score(clasificador3, df3, target3, cv=10, scoring='accuracy')\n",
    "\n",
    "print(\"[Clasificador1] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores1.mean(), scores1.std()))\n",
    "print(\"[Clasificador2] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores2.mean(), scores2.std()))\n",
    "print(\"[Clasificador3] Mean: %0.3f - Standar Deviation: %0.3f\"%(scores3.mean(), scores3.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
