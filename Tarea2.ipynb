{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2 AID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bruno Benkel - 201204111-2\n",
    "- Felipe Chacon - 201303017-3\n",
    "- Diego Wilhelm - 201303059-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import cluster\n",
    "from sklearn import metrics\n",
    "from statsmodels.sandbox.tools.tools_pca import pcasvd\n",
    "from sklearn.neighbors import kneighbors_graph # Técnica de k-Vecino mas cercano\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats, integrate\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hepatitis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Class: DIE, LIVE\n",
    " 2. AGE: 10, 20, 30, 40, 50, 60, 70, 80\n",
    " 3. SEX: male, female\n",
    " 4. STEROID: no, yes\n",
    " 5. ANTIVIRALS: no, yes\n",
    " 6. FATIGUE: no, yes\n",
    " 7. MALAISE: no, yes\n",
    " 8. ANOREXIA: no, yes\n",
    " 9. LIVER BIG: no, yes\n",
    " 10. LIVER FIRM: no, yes\n",
    " 11. SPLEEN PALPABLE: no, yes\n",
    " 12. SPIDERS: no, yes\n",
    " 13. ASCITES: no, yes\n",
    " 14. VARICES: no, yes\n",
    " 15. BILIRUBIN: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00\n",
    " 16. ALK PHOSPHATE: 33, 80, 120, 160, 200, 250\n",
    " 17. SGOT: 13, 100, 200, 300, 400, 500, \n",
    " 18. ALBUMIN: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0\n",
    " 19. PROTIME: 10, 20, 30, 40, 50, 60, 70, 80, 90\n",
    " 20. HISTOLOGY: no, yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/hepatitis.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for i in range(df1.shape[0]):\n",
    "    if('?' in df1.iloc[i].values):\n",
    "        index.append(i)\n",
    "df1 = df1.drop(index)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Sacar la columna Class, que es la que se intenta predecir\n",
    "target1 = df1['Class']\n",
    "df1 = df1.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mat = df1.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Fueron utilizados K-Means, Average Linkage y Mean Shift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=2)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### WARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity = kneighbors_graph(mat, n_neighbors=10, include_self=False)\n",
    "algoritmo = cluster.AgglomerativeClustering(n_clusters=2, linkage='ward',\n",
    "                                           connectivity=connectivity)\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_entrenamiento, H_test, target1_entrenamiento, target1_test = train_test_split(df1, target1, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc1 = StandardScaler()\n",
    "sc1.fit(H_entrenamiento) \n",
    "Z1_entrenamiento = sc1.transform(H_entrenamiento)\n",
    "Z1_test = sc1.transform(H_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred1 = clasificador1.predict(Z1_test)\n",
    "accuracy1 = accuracy_score(target1_test, target1_pred1)\n",
    "precision1 = precision_score(target1_test, target1_pred1, average=\"macro\")\n",
    "F1 = f1_score(target1_test, target1_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred2 = clasificador2.predict(Z1_test)\n",
    "accuracy2 = accuracy_score(target1_test, target1_pred2)\n",
    "precision2 = precision_score(target1_test, target1_pred2, average=\"macro\")\n",
    "F2 = f1_score(target1_test, target1_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z1_entrenamiento, target1_entrenamiento)\n",
    "target1_pred3 = clasificador3.predict(Z1_test)\n",
    "accuracy3 = accuracy_score(target1_test, target1_pred3)\n",
    "precision3 = precision_score(target1_test, target1_pred3, average=\"macro\")\n",
    "F3 = f1_score(target1_test, target1_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = df1.astype(float)\n",
    "df1 = (df1 - df1.mean()) / df1.std()\n",
    "pca = pcasvd(df1, keepdim=0, demean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indian Diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of times pregnant\n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "3. Diastolic blood pressure (mm Hg)\n",
    "4. Triceps skin fold thickness (mm)\n",
    "5. 2-Hour serum insulin (mu U/ml)\n",
    "6. Body mass index (weight in kg/(height in m)^2)\n",
    "7. Diabetes pedigree function\n",
    "8. Age (years)\n",
    "9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/diabetes.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for i in range(df2.shape[0]):\n",
    "    if('?' in df2.iloc[i].values):\n",
    "        index.append(i)\n",
    "df2 = df2.drop(index)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target2 = df2['Class']\n",
    "df2 = df2.drop('Class',axis=1)\n",
    "mat = df2.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=2)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=2)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_entrenamiento, D_test, target2_entrenamiento, target2_test = train_test_split(df2, target2, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc2 = StandardScaler()\n",
    "sc2.fit(D_entrenamiento) \n",
    "Z2_entrenamiento = sc2.transform(D_entrenamiento)\n",
    "Z2_test = sc2.transform(D_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred1 = clasificador1.predict(Z2_test)\n",
    "accuracy1 = accuracy_score(target2_test, target2_pred1)\n",
    "precision1 = precision_score(target2_test, target2_pred1, average=\"macro\")\n",
    "F1 = f1_score(target2_test, target2_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred2 = clasificador2.predict(Z2_test)\n",
    "accuracy2 = accuracy_score(target2_test, target2_pred2)\n",
    "precision2 = precision_score(target2_test, target2_pred2, average=\"macro\")\n",
    "F2 = f1_score(target2_test, target2_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z2_entrenamiento, target2_entrenamiento)\n",
    "target2_pred3 = clasificador3.predict(Z2_test)\n",
    "accuracy3 = accuracy_score(target2_test, target2_pred3)\n",
    "precision3 = precision_score(target2_test, target2_pred3, average=\"macro\")\n",
    "F3 = f1_score(target2_test, target2_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = df2.astype(float)\n",
    "df2 = (df2 - df2.mean()) / df2.std()\n",
    "pca = pcasvd(df2, keepdim=0, demean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Treatment Plant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|N. | Identificador | Descripción |\n",
    "| -- |:---:|:---|\n",
    "| 1 | Q-E       | (input flow to plant)   |\n",
    "| 2 | ZN-E      | (input Zinc to plant) |\n",
    "| 3 | PH-E      | (input pH to plant)  |\n",
    "| 4 | DBO-E     | (input Biological demand of oxygen to plant)  |\n",
    "| 5 | DQO-E     | (input chemical demand of oxygen to plant) |\n",
    "| 6 | SS-E      | (input suspended solids to plant)   |\n",
    "| 7 | SSV-E     | (input volatile supended solids to plant) |\n",
    "| 8 | SED-E     | (input sediments to plant)  |\n",
    "| 9 | COND-E    | (input conductivity to plant)  |\n",
    "|10 | PH-P      | (input pH to primary settler) |\n",
    "|11 | DBO-P     | (input Biological demand of oxygen to primary settler) |\n",
    "|12 | SS-P      | (input suspended solids to primary settler) |\n",
    "|13 | SSV-P     | (input volatile supended solids to primary settler) |\n",
    "|14 | SED-P     | (input sediments to primary settler)  |\n",
    "|15 | COND-P    | (input conductivity to primary settler) |\n",
    "|16 | PH-D      | (input pH to secondary settler)  |\n",
    "|17 | DBO-D     | (input Biological demand of oxygen to secondary settler) |\n",
    "|18 | DQO-D     | (input chemical demand of oxygen to secondary settler) |\n",
    "|19 | SS-D      | (input suspended solids to secondary settler) |\n",
    "|20 | SSV-D     | (input volatile supended solids to secondary settler) |\n",
    "|21 | SED-D     | (input sediments to secondary settler)   |\n",
    "|22 | COND-D    | (input conductivity to secondary settler)  |\n",
    "|23 | PH-S      | (output pH)    |\n",
    "|24 | DBO-S     | (output Biological demand of oxygen) |\n",
    "|25 | DQO-S     | (output chemical demand of oxygen) |\n",
    "|26 | SS-S      | (output suspended solids) |\n",
    "|27 | SSV-S     | (output volatile supended solids)  |\n",
    "|28 | SED-S     | (output sediments)  |\n",
    "|29 | COND-S    | (output conductivity) |\n",
    "|30 | RD-DBO-P  | (performance input Biological demand of oxygen in primary settler) |\n",
    "|31 | RD-SS-P   | (performance input suspended solids to primary settler) |\n",
    "|32 | RD-SED-P  | (performance input sediments to primary settler) |\n",
    "|33 | RD-DBO-S  | (performance input Biological demand of oxygen to secondary settler) |\n",
    "|34 | RD-DQO-S  | (performance input chemical demand of oxygen to secondary settler) |\n",
    "|35 | RD-DBO-G  | (global performance input Biological demand of oxygen) |\n",
    "|36 | RD-DQO-G  | (global performance input chemical demand of oxygen) |\n",
    "|37 | RD-SS-G   | (global performance input suspended solids)  |\n",
    "|38 | RD-SED-G  | (global performance input sediments) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay 13 clases a las que puede pertenecer esto\n",
    "\n",
    "* Class 1: Normal situation\n",
    "* Class 2: Secondary settler problems-1\n",
    "* Class 3: Secondary settler problems-2\n",
    "* Class 4: Secondary settler problems-3\n",
    "* Class 5: Normal situation with performance over the mean\n",
    "* Class 6: Solids overload-1\n",
    "* Class 7: Secondary settler problems-4\n",
    "* Class 8: Storm-1\n",
    "* Class 9: Normal situation with a low influent\n",
    "* Class 10: Storm-2\n",
    "* Class 11: Normal situation\n",
    "* Class 12: Storm-3\n",
    "* Class 13: Solids overload-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('data/water-treatment.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quitar las filas que tengan missing values '?'\n",
    "index = []\n",
    "for j in df3.index:\n",
    "    if('?' in df3.loc[j].values):\n",
    "        index.append(j)\n",
    "df3 = df3.drop(index)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "target3 = np.random.randint(2,13,size=380)\n",
    "#dataset no incluye el atributo de la clase\n",
    "mat = df3.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=13)\n",
    "kmeans.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, kmeans.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmo = cluster.AgglomerativeClustering(linkage=\"average\", \n",
    "                                            affinity=\"cityblock\", n_clusters=13)\n",
    "# Linkage: complete, average, ward\n",
    "# Affinity: “euclidean”, “l1”, “l2”, “manhattan”, \"cityblock\", “cosine”, o ‘precomputed’\n",
    "algoritmo.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, algoritmo.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean-Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ancho_banda = cluster.estimate_bandwidth(mat, quantile=0.15)\n",
    "mean_shift_alg = cluster.MeanShift(bandwidth=ancho_banda, bin_seeding=True)\n",
    "mean_shift_alg.fit(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Silhouette Coefficient: %0.6f\"\n",
    "      % metrics.silhouette_score(mat, mean_shift_alg.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_entrenamiento, W_test, target3_entrenamiento, target3_test = train_test_split(df3, target3, test_size = 0.5, random_state=0)\n",
    "\n",
    "sc3 = StandardScaler()\n",
    "sc3.fit(W_entrenamiento) \n",
    "Z3_entrenamiento = sc3.transform(W_entrenamiento)\n",
    "Z3_test = sc3.transform(W_test)\n",
    "\n",
    "clasificador1 = LinearDiscriminantAnalysis()\n",
    "clasificador1.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred1 = clasificador1.predict(Z3_test)\n",
    "accuracy1 = accuracy_score(target3_test, target3_pred1)\n",
    "precision1 = precision_score(target3_test, target3_pred1, average=\"macro\")\n",
    "F1 = f1_score(target3_test, target3_pred1, average='macro')\n",
    "\n",
    "clasificador2 = QuadraticDiscriminantAnalysis()\n",
    "clasificador2.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred2 = clasificador2.predict(Z3_test)\n",
    "accuracy2 = accuracy_score(target3_test, target3_pred2)\n",
    "precision2 = precision_score(target3_test, target3_pred2, average=\"macro\")\n",
    "F2 = f1_score(target3_test, target3_pred2, average='macro')\n",
    "\n",
    "clasificador3=SVC(kernel=\"linear\", C=0.025)\n",
    "clasificador3.fit(Z3_entrenamiento, target3_entrenamiento)\n",
    "target3_pred3 = clasificador3.predict(Z3_test)\n",
    "accuracy3 = accuracy_score(target3_test, target3_pred3)\n",
    "precision3 = precision_score(target3_test, target3_pred3, average=\"macro\")\n",
    "F3 = f1_score(target3_test, target3_pred3, average='macro')\n",
    "\n",
    "print(\"[Clasificador 1] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy1, precision1, F1))\n",
    "print(\"[Clasificador 2] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy2, precision2, F2))\n",
    "print(\"[Clasificador 3] Accuracy: %0.3f  -  Precision: %0.2f  -  F Score: %0.3f\"%(accuracy3, precision3, F3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = df3.astype(float)\n",
    "df3 = (df3 - df3.mean()) / df3.std()\n",
    "pca = pcasvd(df3, keepdim=0, demean=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
